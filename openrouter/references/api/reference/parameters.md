# LLM Parameters

Sampling parameters that control token generation

The OpenRouter API accepts various sampling parameters that shape how tokens are generated. OpenRouter will default to the values listed below if certain parameters are absent from your request (for example, `temperature` to 1.0). Provider-specific parameters (like `safe_prompt` for Mistral) are passed directly to their respective providers.

## Sampling Parameters

### Temperature

- **Key:** `temperature`
- **Type:** float
- **Range:** 0.0 to 2.0
- **Default:** 1.0

Influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.

### Top P

- **Key:** `top_p`
- **Type:** float
- **Range:** 0.0 to 1.0
- **Default:** 1.0

Limits the model's choices to a percentage of likely tokens: only the top tokens whose probabilities add up to P. A lower value makes the model's responses more predictable, while the default setting allows for a full range of token choices. Think of it like a dynamic Top-K.

### Top K

- **Key:** `top_k`
- **Type:** integer
- **Range:** 0 or above
- **Default:** 0

Limits the model's choice of tokens at each step, making it choose from a smaller set. A value of 1 means the model will always pick the most likely next token, leading to predictable results. By default (0), this setting is disabled, meaning the model considers all choices.

### Frequency Penalty

- **Key:** `frequency_penalty`
- **Type:** float
- **Range:** -2.0 to 2.0
- **Default:** 0.0

Aims to control the repetition of tokens based on how often they appear in the input. It specifically increases the token's penalty by an amount proportional to how often the token has already appeared in the input. Negative values will encourage token reuse.

### Presence Penalty

- **Key:** `presence_penalty`
- **Type:** float
- **Range:** -2.0 to 2.0
- **Default:** 0.0

Adjusts how often the model repeats specific tokens already used in the input. Higher values make such repetition less likely, while negative values do the opposite. Unlike frequency penalty, the token's penalty does not scale with the number of occurrences.

### Repetition Penalty

- **Key:** `repetition_penalty`
- **Type:** float
- **Range:** 0.0 to 2.0
- **Default:** 1.0

Helps to reduce the repetition of tokens from the input. A higher value makes the model less likely to repeat tokens, but too high a value can make the output less coherent.

### Min P

- **Key:** `min_p`
- **Type:** float
- **Range:** 0.0 to 1.0
- **Default:** 0.0

Represents the minimum probability for a token to be considered, relative to the probability of the most likely token.

### Top A

- **Key:** `top_a`
- **Type:** float
- **Range:** 0.0 to 1.0
- **Default:** 0.0

Consider only the top tokens with "sufficiently high" probabilities based on the probability of the most likely token. Think of it like a dynamic Top-P.

## Generation Control Parameters

### Seed

- **Key:** `seed`
- **Type:** integer

If specified, the inferencing will sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed for all models.

### Max Tokens

- **Key:** `max_tokens`
- **Type:** integer
- **Range:** 1 or above

Sets the upper limit for the number of tokens the model can generate in response. The maximum value is the context length minus the prompt length.

### Stop

- **Key:** `stop`
- **Type:** array

Stop generation immediately if the model encounters any token specified in the stop array.

## Advanced Parameters

### Logit Bias

- **Key:** `logit_bias`
- **Type:** map (JSON object)
- **Range:** -100 to 100 per token

Accepts a JSON object that maps tokens (specified by their token ID) to an associated bias value. The bias is added to the logits generated by the model prior to sampling. The exact effect varies per model.

### Logprobs

- **Key:** `logprobs`
- **Type:** boolean

Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned.

### Top Logprobs

- **Key:** `top_logprobs`
- **Type:** integer
- **Range:** 0 to 20

Specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

## Output Formatting

### Response Format

- **Key:** `response_format`
- **Type:** map

Forces the model to produce specific output format. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON. Make sure to also instruct the model to produce JSON via a system or user message.

### Structured Outputs

- **Key:** `structured_outputs`
- **Type:** boolean

If the model can return structured outputs using `response_format` with `json_schema`.

### Verbosity

- **Key:** `verbosity`
- **Type:** enum (`low`, `medium`, `high`, `max`)
- **Default:** `medium`

Constrains the verbosity of the model's response. Lower values produce more concise responses, while higher values produce more detailed and comprehensive responses. For Anthropic models, this maps to `output_config.effort`; `max` requires Claude 4.6 Opus or later.

## Tool Integration

### Tools

- **Key:** `tools`
- **Type:** array

Tool calling parameter, following OpenAI's tool calling request shape. For non-OpenAI providers, it will be transformed accordingly.

### Tool Choice

- **Key:** `tool_choice`
- **Type:** array

Controls which (if any) tool is called by the model. `none` means the model will not call any tool. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools. Or specify a particular tool.

### Parallel Tool Calls

- **Key:** `parallel_tool_calls`
- **Type:** boolean
- **Default:** `true`

Whether to enable parallel function calling during tool use. If true, the model can call multiple functions simultaneously.
